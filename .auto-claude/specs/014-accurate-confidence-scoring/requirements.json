{
  "task_description": "# Accurate Confidence Scoring\n\nCalibrate confidence scores to accurately reflect detection reliability. Currently scores are conservative (40% when actual accuracy may be higher), confusing users.\n\n## Rationale\nConservative confidence scores undermine user trust. Accurate scores help users know when to trust automated detection vs when to verify manually, improving overall experience.\n\n## User Stories\n- As a user, I want confidence scores to accurately reflect detection quality so that I know when to review the extraction\n- As a power user, I want to trust high-confidence detections without manual verification\n\n## Acceptance Criteria\n- [ ] Confidence scores correlate with actual detection accuracy within 10%\n- [ ] Different confidence thresholds for different detection components\n- [ ] Visual indicators clearly communicate confidence levels to users\n- [ ] High-confidence detections proven reliable in testing\n",
  "workflow_type": "feature"
}